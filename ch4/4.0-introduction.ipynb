{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Linear Methods for Classification\n",
    "\n",
    "Since our predictor $G(x)$ takes values in a discrete set $\\mathcal{G}$, we can always divide the input space into a collection of regions labeled according to the classification. We saw in Chapter 2 that the boundaries of these regions can be rough or smooth, depending on the prediction function. For an important class of procedures, these _decision boundaries_ are linear; this is what we it mean by linear methodds for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the connection between features $X$ and __discrete__ outcomes, we \n",
    "could reply on _logit transformation_ as follows: \n",
    "\n",
    "\\begin{align}\n",
    "\\text{Pr}(G=1|X=x) &= \\frac{\\exp(\\beta_0+\\beta^Tx)}{1+\\exp(\\beta_0+\\beta^Tx)},\\\\\n",
    "\\text{Pr}(G=2|X=x) &= \\frac{1}{1+\\exp(\\beta_0+\\beta^Tx)},\\\\\n",
    "\\end{align}\n",
    "\n",
    "where the monotone transformation is the *logit* transformation\n",
    "\n",
    "\\begin{equation}\n",
    "\\log\\frac{p}{1-p},\n",
    "\\end{equation}\n",
    "\n",
    "and in fact we see that\n",
    "\n",
    "\\begin{equation}\n",
    "\\log\\frac{\\text{Pr}(G=1|X=x)}{\\text{Pr}(G=2|X=x)} = \\beta_0 + \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "The decision boundary is the set of points for which the *log-odds* are zero, and this is a hyperplane defined by\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\lbrace x: \\beta_0+\\beta^Tx = 0 \\right\\rbrace.\n",
    "\\end{equation}\n",
    "\n",
    "We will discuss two very popular but different methods that result in \n",
    "linear log-odds or logits: Linear discriminant analysis and linear \n",
    "logistic regression.\n",
    "\n",
    "Once could also classify __discrete__ outcomes without using log likelihood\n",
    "functions, which means we can explicitly model the boundaries between the\n",
    "classes as linear. \n",
    "\n",
    "We will look at two methods that explicitly look for \"separating hyperplanes\".\n",
    "\n",
    "1. The well-known perceptron model of Rosenblatt (1958), with an algorithm that finds a separating hyperplane in the training data, if one exists.\n",
    "2. Vapnik (1996) finds an optimally separating hyperplane if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data.\n",
    "\n",
    "When a separating hyperplane could be found we say it is _linear classficiable_,\n",
    "whereas we need to use _neural network_ to classifiy them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e637eb1f212951f11bbcb105d1b42c01fcba4bbbd2a19d5c9065a96f28919fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
