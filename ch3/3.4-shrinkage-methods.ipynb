{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Shrinkage Methods\n",
    "\n",
    "By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete process— variables are either retained or discarded—it often exhibits high variance, and so doesn’t reduce the prediction error of the full model. Shrinkage methods are more continuous, and don’t suffer as much from high variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ch3 import LinearRegression\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Ridge Regression\n",
    "\n",
    "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{ridge}} = {\\arg\\min}_{\\beta}\\left\\lbrace \\sum_{i=1}^N\\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 +\\lambda\\sum_{j=1}^p \\beta_j^2 \\right\\rbrace,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda \\ge 0$ is a complexity parameter that controls the amount of shrinkage: the larger the $\\lambda$, the greater the amount of shrinkage. The idea of penalizing by the sum-of-squares of the parameters is also used in neural networks, a.k.a. *weight decay* (Chapter 11).\n",
    "\n",
    "An equivalent way to write the ridge problem is to make explicit the size constraint on the parameters, as\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\hat\\beta^{\\text{ridge}} & = {\\arg\\min}_\\beta \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 \\\\ \n",
    "& \\text{ subject to } \\sum_{j=1}^p \\beta_j^2 \\le t,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where $t$ has a one-to-one correspondence with $\\lambda$.\n",
    "\n",
    "When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing a size constraint on the coefficients, this problem is alleviated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and centering\n",
    "\n",
    "The ridge solutions are not equivariant (not comparable)under scaling of the inputs, and so  one normally standardizes the inputs before solving the ridge problem.\n",
    "\n",
    "Also notice that the intercept $\\beta_0$ has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for $Y$; i.e., adding a constant $c$ to each of the targets $y_i$ (i.e. simply shifting) would not simply result in a shift of the predictions by the same constant $c$.\n",
    "\n",
    "It can be shown that the ridge solution can be separated into two parts, after reparametrization using *centered* inputs: Each $x_{ij}$ gets replaced by $x_{ij}-\\bar{x}_j$.\n",
    "1. We estimate $\\beta_0$ by the mean response $\\bar{y} = \\frac{1}{N}\\sum_1^N y_i$.\n",
    "2. The remaining coefficients get estimated by a ridge regression without intercept, using centered $x_{ij}$.\n",
    "\n",
    "Henceforth we assume that this centering has been done, so that the input matrix $\\mathbf{X}$ has $p$ columns rather than $p+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix form\n",
    "\n",
    "The equation (1) could be written in the following matrix form: \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(\\lambda) = (\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta) + \\lambda\\beta^T\\beta\n",
    "\\end{equation}\n",
    "\n",
    "The ridge regression solutions are easily seen to be\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{ridge}} = \\left( \\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I} \\right)^{-1}\\mathbf{X}^T\\mathbf{y},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{I}$ is the $p\\times p$ identity matrix. Notice that the ridge solution is again a linear function of $\\mathbf{y}$ by the choice of quadratic penalty $\\beta^T\\beta$, resulting in addition of a positive constant to the diagonal of $\\mathbf{X}^T\\mathbf{X}$ before inversion. _This makes the problem nonsingular, even if $\\mathbf{X}^T\\mathbf{X}$ is not of full rank_. It was actually the main motivation for ridge regression when it was first introduced in statistics (Hoerl and Kennard, 1970).\n",
    "\n",
    "__Remark__: It is all about penalty. In equation (1), the\n",
    "penalty was added via $\\lambda$ that corresponds to $t$ in equation (2) one-to-one. Therefore, our new `OLS` estimation should have an additional parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X :np.ndarray, Y :np.ndarray, lam :float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Y need to be centered by subtracting the mean \\\\\n",
    "    beta_intercept = Y.mean() \\\\ \n",
    "    X need to be normalized \\\\ \n",
    "    lam: value of lambda \n",
    "    The coefficients could be estimated via equation (4)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Singular Value Decomposition (SVD) \n",
    "\n",
    "Before, we implement the shrinkage method, we need to review _singular value decomposition_ (SVD). \n",
    "\n",
    "![svd](https://uploads-ssl.webflow.com/5b1d427ae0c922e912eda447/5feb62f53532cb257a8f901d_open_compressed.jpg)\n",
    "\n",
    "Let $ X $ be an $ m \\times n $ matrix of rank $ p $.\n",
    "\n",
    "Necessarily, $ p \\leq \\min(m,n) $.\n",
    "\n",
    "In this lecture, we’ll think of $ X $ as a matrix of **data**.\n",
    "\n",
    "- each column is an **individual** – a time period or person, depending on the application  \n",
    "- each row is a **random variable** describing an attribute of a time period or a person, depending on the application  \n",
    "\n",
    "\n",
    "We’ll be interested in  two  cases\n",
    "\n",
    "- A **short and fat** case in which $ m << n $, so that there are many more columns (individuals) than rows (attributes).  \n",
    "- A  **tall and skinny** case in which $ m >> n $, so that there are many more rows  (attributes) than columns (individuals).  \n",
    "\n",
    "\n",
    "We’ll apply a **singular value decomposition** of $ X $ in both situations.\n",
    "\n",
    "In the first case in which there are many more individuals $ n $ than attributes $ m $, we learn sample moments of  a joint distribution  by taking averages  across observations of functions of the observations.\n",
    "\n",
    "In this $ m < < n $ case,  we’ll look for **patterns** by using a **singular value decomposition** to do a **principal components analysis** (PCA).\n",
    "\n",
    "In the $ m > > n $  case in which there are many more attributes $ m $ than individuals $ n $, we’ll proceed in a different way.\n",
    "\n",
    "A **singular value decomposition** of an $ m \\times n $ matrix $ X $ of rank $ p \\leq \\min(m,n) $ is\n",
    "\n",
    "$$\n",
    "X  = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "UU^T &  = I  &  \\quad U^T U = I \\cr    \n",
    "VV^T & = I & \\quad V^T V = I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $ U $ is an $ m \\times m $ matrix whose columns are eigenvectors of $ X^T X $  \n",
    "- $ V $ is an $ n \\times n $ matrix whose columns are eigenvectors of $ X X^T $  \n",
    "- $ \\Sigma $ is an $ m \\times n $ matrix in which the first $ p $ places on its main diagonal are positive numbers $ \\sigma_1, \\sigma_2, \\ldots, \\sigma_p $ called **singular values**; remaining entries of $ \\Sigma $ are all zero  \n",
    "- The $ p $ singular values are square roots of the eigenvalues of the $ m \\times m $ matrix  $ X X^T $ and the $ n \\times n $ matrix $ X^T X $  \n",
    "- When $ U $ is a complex valued matrix, $ U^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ U $, meaning that\n",
    "  $ U_{ij}^T $ is the complex conjugate of $ U_{ji} $.  \n",
    "- Similarly, when $ V $ is a complex valued matrix, $ V^T $ denotes the **conjugate-transpose** or **Hermitian-transpose** of $ V $  \n",
    "\n",
    "\n",
    "In what is called a **full** SVD, the  shapes of $ U $, $ \\Sigma $, and $ V $ are $ \\left(m, m\\right) $, $ \\left(m, n\\right) $, $ \\left(n, n\\right) $, respectively.\n",
    "\n",
    "There is also an alternative shape convention called an **economy** or **reduced** SVD .\n",
    "\n",
    "Thus, note that because we assume that $ X $ has rank $ p $, there are only $ p $ nonzero singular values, where $ p=\\textrm{rank}(X)\\leq\\min\\left(m, n\\right) $.\n",
    "\n",
    "A **reduced** SVD uses this fact to express $ U $, $ \\Sigma $, and $ V $ as matrices with shapes $ \\left(m, p\\right) $, $ \\left(p, p\\right) $, $ \\left( n, p\\right) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = \n",
      "           0         1         2         3         4\n",
      "0  0.551907 -0.575104 -0.542202 -0.180746 -0.194940\n",
      "1  0.248943  0.677926 -0.154663 -0.598767 -0.309840\n",
      "2  0.617680 -0.107424  0.775913 -0.033464 -0.061408\n",
      "3  0.394829  0.411676 -0.236287  0.771295 -0.154614\n",
      "4  0.309850  0.169279 -0.155640 -0.113105  0.915599\n",
      "S = \n",
      "           0\n",
      "0  1.603112\n",
      "1  0.795896\n",
      "V = \n",
      "           0         1\n",
      "0  0.565994  0.824410\n",
      "1  0.824410 -0.565994\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(5,2)  # tall-skinny\n",
    "U, S, V = np.linalg.svd(X,full_matrices=True)  # full SVD\n",
    "Uhat, Shat, Vhat = np.linalg.svd(X,full_matrices=False) # economy SVD\n",
    "print(\"U = \\n\", pd.DataFrame(U))  # 5 by 5 \n",
    "print(\"S = \\n\", pd.DataFrame(S))  # 2 by 1 \n",
    "print(\"V = \\n\", pd.DataFrame(V))  # 2 by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = \n",
      "           0         1\n",
      "0  0.551907 -0.575104\n",
      "1  0.248943  0.677926\n",
      "2  0.617680 -0.107424\n",
      "3  0.394829  0.411676\n",
      "4  0.309850  0.169279\n",
      "S = \n",
      "           0\n",
      "0  1.603112\n",
      "1  0.795896\n",
      "V = \n",
      "           0         1\n",
      "0  0.565994  0.824410\n",
      "1  0.824410 -0.565994\n"
     ]
    }
   ],
   "source": [
    "print(\"U = \\n\", pd.DataFrame(Uhat))  # 5 by 2\n",
    "print(\"S = \\n\", pd.DataFrame(Shat))  # 2 by 1 \n",
    "print(\"V = \\n\", pd.DataFrame(Vhat))  # 2 by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of splitted dataset is: (67, 8)\n",
      "The shape of splitted dataset is: (30, 8)\n",
      "The shape of splitted dataset is: (67, 1)\n",
      "The shape of splitted dataset is: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.normalize_dataset()\n",
    "lr.split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will run the regression with shrinkage methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e637eb1f212951f11bbcb105d1b42c01fcba4bbbd2a19d5c9065a96f28919fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
