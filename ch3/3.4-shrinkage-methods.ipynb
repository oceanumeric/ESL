{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Shrinkage Methods\n",
    "\n",
    "By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete process— variables are either retained or discarded—it often exhibits high variance, and so doesn’t reduce the prediction error of the full model. Shrinkage methods are more continuous, and don’t suffer as much from high variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ch3 import LinearRegression, SVD\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 Ridge Regression\n",
    "\n",
    "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{ridge}} = {\\arg\\min}_{\\beta}\\left\\lbrace \\sum_{i=1}^N\\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 +\\lambda\\sum_{j=1}^p \\beta_j^2 \\right\\rbrace,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda \\ge 0$ is a complexity parameter that controls the amount of shrinkage: the larger the $\\lambda$, the greater the amount of shrinkage. The idea of penalizing by the sum-of-squares of the parameters is also used in neural networks, a.k.a. *weight decay* (Chapter 11).\n",
    "\n",
    "An equivalent way to write the ridge problem is to make explicit the size constraint on the parameters, as\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\hat\\beta^{\\text{ridge}} & = {\\arg\\min}_\\beta \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 \\\\ \n",
    "& \\text{ subject to } \\sum_{j=1}^p \\beta_j^2 \\le t,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where $t$ has a one-to-one correspondence with $\\lambda$.\n",
    "\n",
    "When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing a size constraint on the coefficients, this problem is alleviated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and centering\n",
    "\n",
    "The ridge solutions are not equivariant (not comparable)under scaling of the inputs, and so  one normally standardizes the inputs before solving the ridge problem.\n",
    "\n",
    "Also notice that the intercept $\\beta_0$ has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for $Y$; i.e., adding a constant $c$ to each of the targets $y_i$ (i.e. simply shifting) would not simply result in a shift of the predictions by the same constant $c$.\n",
    "\n",
    "It can be shown that the ridge solution can be separated into two parts, after reparametrization using *centered* inputs: Each $x_{ij}$ gets replaced by $x_{ij}-\\bar{x}_j$.\n",
    "1. We estimate $\\beta_0$ by the mean response $\\bar{y} = \\frac{1}{N}\\sum_1^N y_i$.\n",
    "2. The remaining coefficients get estimated by a ridge regression without intercept, using centered $x_{ij}$.\n",
    "\n",
    "Henceforth we assume that this centering has been done, so that the input matrix $\\mathbf{X}$ has $p$ columns rather than $p+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix form\n",
    "\n",
    "The equation (1) could be written in the following matrix form: \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(\\lambda) = (\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta) + \\lambda\\beta^T\\beta\n",
    "\\end{equation}\n",
    "\n",
    "The ridge regression solutions are easily seen to be\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{ridge}} = \\left( \\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I} \\right)^{-1}\\mathbf{X}^T\\mathbf{y},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{I}$ is the $p\\times p$ identity matrix. Notice that the ridge solution is again a linear function of $\\mathbf{y}$ by the choice of quadratic penalty $\\beta^T\\beta$, resulting in addition of a positive constant to the diagonal of $\\mathbf{X}^T\\mathbf{X}$ before inversion. _This makes the problem nonsingular, even if $\\mathbf{X}^T\\mathbf{X}$ is not of full rank_. It was actually the main motivation for ridge regression when it was first introduced in statistics (Hoerl and Kennard, 1970).\n",
    "\n",
    "__Remark__: It is all about penalty. In equation (1), the\n",
    "penalty was added via $\\lambda$ that corresponds to $t$ in equation (2) one-to-one. Therefore, our new `OLS` estimation should have an additional parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X :np.ndarray, Y :np.ndarray, lam :float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Y need to be centered by subtracting the mean \\\\\n",
    "    beta_intercept = Y.mean() \\\\ \n",
    "    X need to be normalized \\\\ \n",
    "    lam: value of lambda \n",
    "    The coefficients could be estimated via equation (4)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and SVD\n",
    "\n",
    "The SVD of the centered input matrix $\\mathbf{X}$ gives us some additional insight into the nature of ridge regression. The SVD of the $N\\times p$ matrix $\\mathbf{X}$ has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{U\\Sigma V}^T,\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* $\\mathbf{U}$ is $N\\times p$ orthogonal matrix, with the columns of $\\mathbf{U}$ spanning the $\\text{col}(\\mathbf{X})$\n",
    "* $\\mathbf{V}$ is $p\\times p$ orthogonal matrix, with the columns of $\\mathbf{V}$ spanning the $\\text{row}(\\mathbf{X})$\n",
    "* $\\mathbf{\\Sigma}$ is a $p\\times p$ diagonal matrix, with diagonal entries  \n",
    "$d_1 \\ge d_2 \\ge \\cdots \\ge d_p \\ge 0$ called the singular values of $\\mathbf{X}$.\n",
    "* If one or more values $d_j = 0$, $\\mathbf{X}$ is singular.\n",
    "\n",
    "Using the SVD we can write the least squares fitted vector as\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbf{X}\\hat\\beta^{\\text{ls}} &= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "&= \\mathbf{UU}^T\\mathbf{y}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Note\n",
    "* $\\mathbf{U}^T\\mathbf{y}$ are the coordinates of $\\mathbf{y}$ w.r.t. the orthonormal basis $\\mathbf{U}$.\n",
    "* the similarity with QR decomposition; $\\mathbf{Q}$ and $\\mathbf{U}$ are generally different orthogonal bases for $\\text{col}(\\mathbf{X})$.\n",
    "\n",
    "Now the ridge solutions are\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbf{X}\\hat\\beta^{\\text{ridge}} &= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n",
    "&= \\mathbf{U \\Sigma}(\\mathbf{\\Sigma}^2 + \\lambda\\mathbf{I})^{-1}\\mathbf{\\Sigma U}^T\\mathbf{y} \\\\\n",
    "&= \\sum_{j=1}^p \\mathbf{u}_j \\frac{d_j^2}{d_j^2+\\lambda}\\mathbf{u}_j^T\\mathbf{y},\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\mathbf{u}_j$ are the columns of $\\mathbf{U}$. The ridge solution then shrinks these coordinates by the factors $d_j^2/(d_j^2+\\lambda)$. This means that a greater amount of shrinkage is applied to the coordinates of basis vectors with smaller $d_j^2$. Then what does a small value of $d_j^2$ mean?\n",
    "\n",
    "In our illustration, we have shown that the matrix $D$ scaled the unit circle. When $d_j$ is small, it means that it will scale the effect. _Since the data matrix $X$ speaks for itself, we could correspond to it by shrinking\n",
    "its coefficients._\n",
    "\n",
    "Now, we will plot the error in terms of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e637eb1f212951f11bbcb105d1b42c01fcba4bbbd2a19d5c9065a96f28919fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
