{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Shrinkage Methods\n",
    "\n",
    "By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete process— variables are either retained or discarded—it often exhibits high variance, and so doesn’t reduce the prediction error of the full model. Shrinkage methods are more continuous, and don’t suffer as much from high variability.\n",
    "\n",
    "## 3.4.1 Ridge Regression\n",
    "\n",
    "Ridge regression shrinks the regression coefficients by imposing a penalty on their size. The ridge coefficients minimize a penalized residual sum of squares,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{ridge}} = {\\arg\\min}_{\\beta}\\left\\lbrace \\sum_{i=1}^N\\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 +\\lambda\\sum_{j=1}^p \\beta_j^2 \\right\\rbrace,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda \\ge 0$ is a complexity parameter that controls the amount of shrinkage: the larger the $\\lambda$, the greater the amount of shrinkage. The idea of penalizing by the sum-of-squares of the parameters is also used in neural networks, a.k.a. *weight decay* (Chapter 11).\n",
    "\n",
    "An equivalent way to write the ridge problem is to make explicit the size constraint on the parameters, as\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\hat\\beta^{\\text{ridge}} & = {\\arg\\min}_\\beta \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 \\\\ \n",
    "& \\text{ subject to } \\sum_{j=1}^p \\beta_j^2 \\le t,\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where $t$ has a one-to-one correspondence with $\\lambda$.\n",
    "\n",
    "When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing a size constraint on the coefficients, this problem is alleviated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
