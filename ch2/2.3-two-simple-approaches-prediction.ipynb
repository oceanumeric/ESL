{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors\n",
    "\n",
    "\n",
    "In this section we develop two simple but powerful prediction methods: \n",
    "the linear model fit by least squares and the k-nearest-neighbor prediction rule. \n",
    "The linear model makes _huge assumptions about structure and yields stable \n",
    "but possibly inaccurate predictions_. The method of k-nearest neighbors \n",
    "makes _very mild structural assumptions: its predictions are often accurate \n",
    "but can be unstable_. One should try to understand:\n",
    "\n",
    "* __what does a stable prediction mean?__\n",
    "* __what does an accurate preciction mean?__\n",
    "* __how could we measure them?__\n",
    "\n",
    "__Here is the answer__:\n",
    "\n",
    "> The linear decision boundary from least squares is very smooth, and apparently stable to fit. It does appear to rely heavily on the assumption that a linear decision boundary is appropriate. In language we will develop shortly, it has low variance and potentially high bias. On the other hand, the ùëò-nearest-neighbor procedures do not appear to rely on any stringent assumptions about the underlying data, and can adapt to any situation. However, any particular subregion of the decision boundary depends on a handful of input points and their particular positions, and is thus wiggly and unstable-high variance and low bias.\n",
    "\n",
    "_Here is an illustration_:\n",
    "\n",
    "![bias-variance](https://i.stack.imgur.com/AF3UE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Linear Models and Least Squares\n",
    "\n",
    "For the following multivariate linear regression model,\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\begin{bmatrix}\n",
    "        y_1 \\\\\n",
    "        y_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        y_t \\\\\n",
    "        \\vdots  \\\\ \n",
    "        y_N\n",
    "    \\end{bmatrix} = \\begin{bmatrix}\n",
    "        1 & x_{12} & \\cdots & x_{1k} \\\\\n",
    "        1 & x_{22} & \\cdots & x_{2k} \\\\\n",
    "        \\vdots & \\vdots & \\vdots \\\\\n",
    "        1 & x_{t2} & \\cdots & x_{tk} \\\\ \n",
    "        \\vdots & \\vdots & \\vdots \\\\\n",
    "        1 & x_{N2} & \\cdots & x_{Nk} \n",
    "    \\end{bmatrix} \\begin{bmatrix}\n",
    "        \\beta_1 \\\\ \n",
    "        \\beta_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\beta_t \\\\ \n",
    "        \\vdots \\\\\n",
    "        \\beta_k\n",
    "    \\end{bmatrix} + \\begin{bmatrix}\n",
    "        u_1 \\\\\n",
    "        u_2 \\\\\n",
    "        \\vdots \\\\\n",
    "        u_t \\\\\n",
    "        \\vdots  \\\\ \n",
    "        u_N\n",
    "    \\end{bmatrix}\n",
    "\\end{equation*} \n",
    "$$\n",
    "\n",
    "or simply:\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = X \\beta + u \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "where, $Y$ is a $N \\times 1$ vector, $X$ is a $N \\times k$ matrix, and \n",
    "$\\beta$ is a $k \\times 1$ vector. \n",
    "With assumptions:\n",
    "\\begin{align*}\n",
    "    & E(u)  = 0 \\ \\ \\ \\ \n",
    "    V(u)  = \\sigma^2 I \\\\ \n",
    "    & \\text{X is fixed} \\ \\ \\ \\ \n",
    "    \\text{X  has full rank k (or $k < N$)} \n",
    "\\end{align*}\n",
    "\n",
    "The method of OLS estimation chooses the values for $\\beta_{1}, \\beta_{2}, \\ldots, \\beta_{k}$, denoted $\\hat{\\beta}_{1}, \\hat{\\beta}_{2}, \\ldots, \\hat{\\beta}_{k}$, that minimize the residuals sum of squares :\n",
    "$$\n",
    "R S S=\\sum_{t=1}^{T} \\hat{u}_{t}^{2}\n",
    "$$\n",
    "In matrix form this translates to:\n",
    "$$\n",
    "\\text { choose } \\hat{\\beta} \\text { to minimize } R S S=\\hat{u}^{\\prime} \\hat{u}\n",
    "$$\n",
    "where $\\hat{u}=y-X \\hat{\\beta}$. Now,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R S S &=\\hat{u}^{\\prime} \\hat{u}=(y-X \\hat{\\beta})^{\\prime}(y-X \\hat{\\beta}) \\\\\n",
    "&=\\left(y^{\\prime}-\\hat{\\beta}^{\\prime} X^{\\prime}\\right)(y-X \\hat{\\beta}) \\\\\n",
    "&=y^{\\prime} y-y^{\\prime} X \\hat{\\beta}-\\hat{\\beta}^{\\prime} X^{\\prime} y+\\hat{\\beta}^{\\prime} X^{\\prime} X \\hat{\\beta} \\\\\n",
    "&=y^{\\prime} y-2 y^{\\prime} X \\hat{\\beta}+\\hat{\\beta}^{\\prime} X^{\\prime} X \\hat{\\beta}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where the last line follows because $\\hat{\\beta}^{\\prime} X^{\\prime} y$ is a scalar and hence equal to $\\left(\\hat{\\beta}^{\\prime} X^{\\prime} y\\right)^{\\prime}=$ $y^{\\prime} X \\hat{\\beta}$\n",
    "The first order conditions for minimizing $R S S$ is:\n",
    "$$\n",
    "\\frac{\\partial R S S}{\\partial \\hat{\\beta}}=0,\n",
    "$$\n",
    "and to obtain this $k$ dimensional vector of partial derivatives we will require the following two results from the appendix\n",
    "$$\n",
    "\\frac{\\partial A b}{\\partial b}=A^{\\prime} \\quad ; \\quad \\frac{\\partial b^{\\prime} A b}{\\partial b}=2 A b,\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "\\frac{\\partial R S S}{\\partial \\hat{\\beta}}=-2 X^{\\prime} y+2 X^{\\prime} X \\hat{\\beta}\n",
    "$$\n",
    "Equating to 0 gives\n",
    "$$\n",
    "X^{\\prime} X \\hat{\\beta}=X^{\\prime} y,\n",
    "$$\n",
    "and solving yields the OLS estimator\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta}=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} y .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Note that $\\left(X^{\\prime} X\\right)^{-1}$ must exist given the assumption that $X$ has full column rank $k$. Also, note that second order conditions for a minimum are satisfied:\n",
    "$$\n",
    "\\frac{\\partial^{2} R S S}{\\partial \\hat{\\beta} \\partial \\hat{\\beta}^{\\prime}}=2 X^{\\prime} X>0 \\text { (i.e. is a positive definite matrix) }\n",
    "$$\n",
    "We call $\\hat{y}=X \\hat{\\beta}$ the fitted values from the estimated regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Nearest-Neighbor Methods\n",
    "\n",
    "Nearest-neighbor methods use those observations in the training set \n",
    "$\\tau$ closest in input space to x to form $\\hat{Y}$. Specifically, \n",
    "the k-nearest neighbor fit for $\\hat{Y}$ is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{Y} = \\frac{1}{k} = \\sum_{x_i \\in N_k(x)} y_i\n",
    "\\end{equation}\n",
    "\n",
    "where $N_k(x)$ is the neighborhood of x defined by the $k$ closest points $x_i$ \n",
    "in the training sample. Closeness implies a metric, which for the moment \n",
    "we assume is Euclidean distance. So, in words, we find the $k$ observations \n",
    "with $x_i$ closest to x in input space, and average their responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using simulated data to understand thosw two models\n",
    "\n",
    "Now, we will simulate a dataset to check how those two models will predict.\n",
    "The data is generated with following steps:\n",
    "\n",
    "1. Generate 10 means $m_k$ from a bivariate Gaussian for each class  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "m_k \\sim \\begin{cases}\n",
    "N((1,0)^T, \\mathbf{I}) & \\text{ for BLUE}, \\\\\n",
    "N((0,1)^T, \\mathbf{I}) & \\text{ for ORANGE}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "  \n",
    "2. For each class, we generate 100 observations as follows:\n",
    "  1. Pick an $m_k$ at random with probability 1/10\n",
    "  2. Generate $x_i \\sim N(m_k, \\mathbf{I}/5)$\n",
    "\n",
    "\n",
    "__Remark__: all code were encapsulated in the file [ch2_osl](ch2_osl.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e637eb1f212951f11bbcb105d1b42c01fcba4bbbd2a19d5c9065a96f28919fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
