{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6  Statistical Models, Supervised Learning and Function Approximation\n",
    "\n",
    "### Review\n",
    "\n",
    "Our goal is to find a useful approximation $\\hat{f}(x)$ to $f(x)$ that underlies the predictive relationship between the inputs and outputs.\n",
    "\n",
    "In the theoretical setting of $\\S$ 2.4, we saw that\n",
    "1. for a quantitative response, squared error loss lead us to the regression function  \n",
    "\n",
    "  \\begin{equation}\n",
    "  f(x) = \\text{E}(Y|X=x).\n",
    "  \\end{equation}\n",
    "\n",
    "2. The kNNs can be viewed as direct estimates of this conditional expectation,\n",
    "3. but kNNs can fail at least two ways:\n",
    "  * If the dimension of the input space is high, the nearest neighbors need not be close to the target point, and can result in large errors,\n",
    "  * if special structure is known to exist, this can be used to reduce both the bias and the variance of the estimates.\n",
    "  \n",
    "> We anticipate using other classes of models for $f(x)$, in many cases specifically designed to overcome the dimensionality problems, and here we discuss a framework for incorporating them into the prediction problem.\n",
    "\n",
    "__Remark__: when you have more informtion, you model will work better. But, what\n",
    "is information:\n",
    "\n",
    "* __data__\n",
    "* __model__\n",
    "\n",
    "If you know both of them, it will be great. Otherwise, have more data and\n",
    "_guess a model_ might be still helpful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.1. A Statistical Model for the Joint Distribution $\\text{Pr}(X,Y)$\n",
    "\n",
    "### The additive error model\n",
    "Suppose in fact that our data arose from a statistical model\n",
    "\n",
    "\\begin{equation}\n",
    "Y = f(X) + \\epsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where the random error $\\epsilon$ has $\\text{E}(\\epsilon)=0$ and is independent of $X$.\n",
    "\n",
    "Note that for this model,\n",
    "\n",
    "\\begin{equation}\n",
    "f(x)=\\text{E}(Y|X=x),\n",
    "\\end{equation}\n",
    "\n",
    "and in fact the conditional distribution $\\text{Pr}(Y|X)$ depends on $X$ only through the conditional mean $f(x)$.\n",
    "\n",
    "The additive error model is a useful approximation to the truth. For most systems \n",
    "the input-output pairs $(X,Y)$ will not have a deterministic relationship $Y=f(X)$. \n",
    "Generally there will be other unmeasured variables that also contribute to $Y$, including measurement error.\n",
    "\n",
    "The additive model assumes that we can capture all these departures from a \n",
    "deterministic relationship via the error $\\epsilon$:\n",
    "\n",
    "* __do what you can do with your data__\n",
    "* __leave those unmeasurable things into an assumption of an error term__\n",
    "\n",
    "#### Where the deterministic rules\n",
    "\n",
    "For some problems a deterministic relationship does hold. Many of the \n",
    "classification problems studied in machine learning are of this form, \n",
    "where the response surface can be thought of as a colored map defined in $\\mathbb{R}^p$.\n",
    "\n",
    "The training data consist of colored examples from the map $\\{x_i,g_i\\}$, and \n",
    "the goal is to be able to color any point. Here the function is \n",
    "deterministic, and the randomness enters through the $x$ location of the training points.\n",
    "\n",
    "For the moment we will not pursue such problems, but will see that they can \n",
    "be handled by techniques appropriate for the error-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The i.i.d. assumption\n",
    "\n",
    "The assumption in the above additive error model that the errors are i.i.d. is not strictly necessary, but seems to be the back of our mind when we average squared errors uniformly in our EPE criterion.\n",
    "\n",
    "With such a model it becomes natural to use least squares as a data criterion for model estimation as in the linear model\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{Y} = \\hat\\beta_0 + \\sum_{j=1}^p X_j \\hat\\beta_j.\n",
    "\\end{equation}\n",
    "\n",
    "Simple modifications can be made to avoid the independence assumption; e.g., we can have\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(Y|X=x)=\\sigma(x),\n",
    "\\end{equation}\n",
    "\n",
    "and now both the mean and variance depend on $X$.\n",
    "\n",
    "In general $\\text{Pr}(Y|X)$ can depend on X in complicated ways, but the additive error model precludes these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For qualitative outputs\n",
    "\n",
    "So far we have concentrated on the quantitative response.\n",
    "\n",
    "Additive error models are typically not used for qualitative outputs $G$; in this case the target function $p(X)$ _is_ the conditional density $\\text{Pr}(G|X)$, and this is modeled directly.\n",
    "\n",
    "For example, for two-class data, it is often reasonable to assume that the data arise from independent binary trials, with the probability of one particular outcome being $p(X)$, and the other $1 − p(X)$. Thus if $Y$ is the 0–1 coded version of $G$, then\n",
    "\\begin{equation}\n",
    "\\text{E}(Y |X = x) = p(x),\n",
    "\\end{equation}\n",
    "\n",
    "but the variance depends on $x$ as well: $\\text{Var}(Y |X = x) = p(x)\\left(1 − p(x)\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.6.3. Function Approximation\n",
    "\n",
    "Here\n",
    "* the data pairs $\\{x_i, y_i\\}$ are viewed as points in a $(p+1)$-dimensional Euclidean space.\n",
    "* The function $f(x)$ has domain eqaul to the $p$-dimensional input subspace (or $\\mathbb{R}^p$ for convenience), and\n",
    "* $f$ is related to the data via a model such as $y_i=f(x_i)+\\epsilon$.\n",
    "\n",
    "The goal is to obtain a useful approximation to $f(x)$ for all $x$ in some region of $\\mathbb{R}^p$, given the representations in $\\mathcal{T}$.\n",
    "\n",
    "> Although somewhat less glamorous than the learning paradigm, treating supervised learning as a problem in function approximation encourages the geometrical concepts of Euclidean spaces and mathematical concepts of probabilsitic inference to be applied to the problem. This is the approach taken in this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Associated parameters and basis expansions\n",
    "\n",
    "Many of the approximations we will encounter have associated a set of parameters $\\theta$ that can be modified to suit the data at hand. For example, the linear model\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = x^T\\beta\n",
    "\\end{equation}\n",
    "\n",
    "has $\\theta=\\beta$.\n",
    "\n",
    "Another class of useful approximators can be expressed as _linear basis expansions_\n",
    "\\begin{equation}\n",
    "f_\\theta(x) = \\sum_{k=1}^K h_k(x)\\theta_k,\n",
    "\\end{equation}\n",
    "where the $h_k$ are a suitable set of functions or transformations of the input vector $x$. Traditional examples are polynomial and trigonometric expansions, where for example $h_k$ might be $x_1^2$, $x_1x_2^2$, $\\text{cos}(x_1)$ and so on.\n",
    "\n",
    "We also encounter nonlinear expansions, such as the sigmoid transformation common to neural network models,\n",
    "\\begin{equation}\n",
    "h_k(x) = \\frac{1}{1+\\text{exp}(-x^T\\beta_k)}.\n",
    "\\end{equation}\n",
    "\n",
    "### Least squares again\n",
    "We can use least squares to estimate the parameters $\\theta$ in $f_\\theta$ as we did for the linear model, by minimizing the residual sum-of-squares\n",
    "\\begin{equation}\n",
    "\\text{RSS}(\\theta) = \\sum_{i=1}^N\\left(y_i-f_\\theta(x_i)\\right)^2\n",
    "\\end{equation}\n",
    "as a function of $\\theta$. This seems a reasonable criterion for an additive error model.\n",
    "\n",
    "In terms of function approximation, we imagine our parametrized function as a surface in $p+1$ space, and what we observe are noisy realizations from it. This is easy to visualize when $p=2$ and the vertical coordinate in the output $y$, as in FIGURE 2.10. The noise is in the output coordinate, so we find the set of parameters such that the fitted surface gets as close to the observed points as possible, where close is measured by the sum of squared vertical errors in $\\text{RSS}(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear model we get a simple closed form solution to the minimization problem. This is also true for the basis function methods, if the basis functions themselves do not have any hidden parameters. Otherwise the solution requires either iterative methods or numerical optimization.\n",
    "\n",
    "While least squares is generally very convenient, it is not the only criterion used and in some cases would not make much sense. A more general principle for estimation is _maximum likelihood estimation_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood estimation\n",
    "\n",
    "Suppose we have a random sample $y_i, i=1,\\cdots,N$ from a density $\\text{Pr}_\\theta(y)$ indexed by some parameters $\\theta$. The log-probability of the observed sample is\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\theta) = \\sum_{i=1}^N\\log\\text{Pr}_\\theta(y_i).\n",
    "\\end{equation}\n",
    "\n",
    "> The principle of maximum likelihood assumes that the most reasonable values for $\\theta$ are those for which the probability of the observed sample is largest.\n",
    "\n",
    "#### Least squares = ML with Gaussian errors\n",
    "Least squares for the additive error model $Y = f_\\theta(X) + \\epsilon$, with $\\epsilon \\sim N(0, \\sigma^2)$, is equivalent to maximum likelihood using the conditional likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(Y|X,\\theta) = N(f_\\theta(X), \\sigma^2).\n",
    "\\end{equation}\n",
    "\n",
    "So although the additional assumption of normality seems more restrictive, the results are the same. The log-likelihood of the data is\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\theta) = -\\frac{N}{2}\\log(2\\pi) - N\\log\\sigma - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N \\left( y_i - f_\\theta(x_i) \\right)^2,\n",
    "\\end{equation}\n",
    "\n",
    "and the only term involving $\\theta$ is the last, which is $\\text{RSS}(\\theta)$ up to a scalar negative multiplier.\n",
    "\n",
    "#### Multinomial likelihood for a qualitative output $G$\n",
    "\n",
    "A more interesting example is the multinomial likelihood for the regression function $\\text{Pr}(G|X)$ for a qualitative output $G$.\n",
    "\n",
    "Suppose we have a model\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(G=\\mathcal{G}_k|X=x) = p_{k,\\theta}(x), k=1,\\cdots,K\n",
    "\\end{equation}\n",
    "\n",
    "for the conditional density of each class given $X$, indexed by the parameter \n",
    "vector $\\theta$. Then the log-likelihood (also referred to as the cross-entropy) is\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\theta) = \\sum_{i=1}^N\\log p_{g_i,\\theta}(x_i),\n",
    "\\end{equation}\n",
    "\n",
    "and when maximized it delivers values of $\\theta$ that best conform \n",
    "with the data in the likelihood sense.\n",
    "\n",
    "__Once you had the distibution of the error, you can estimate anything with\n",
    "maximum likelihood estimation__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e637eb1f212951f11bbcb105d1b42c01fcba4bbbd2a19d5c9065a96f28919fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
